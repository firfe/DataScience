{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>02 28, 2014</td>\n",
       "      <td>A2IBPI20UZIR0U</td>\n",
       "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
       "      <td>good</td>\n",
       "      <td>1393545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>5</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>03 16, 2013</td>\n",
       "      <td>A14VAT5EAX3D9S</td>\n",
       "      <td>Jake</td>\n",
       "      <td>Jake</td>\n",
       "      <td>1363392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>08 28, 2013</td>\n",
       "      <td>A195EZSQDW3E21</td>\n",
       "      <td>Rick Bennette \"Rick Bennette\"</td>\n",
       "      <td>It Does The Job Well</td>\n",
       "      <td>1377648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>02 14, 2014</td>\n",
       "      <td>A2C00NNG1ZQQG2</td>\n",
       "      <td>RustyBill \"Sunday Rocker\"</td>\n",
       "      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n",
       "      <td>1392336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>02 21, 2014</td>\n",
       "      <td>A94QU4C90B1AX</td>\n",
       "      <td>SEAN MASLANKA</td>\n",
       "      <td>No more pops when I record my vocals.</td>\n",
       "      <td>1392940800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>So good that I bought another one.  Love the h...</td>\n",
       "      <td>12 21, 2012</td>\n",
       "      <td>A2A039TZMZHH9Y</td>\n",
       "      <td>Bill Lewey \"blewey\"</td>\n",
       "      <td>The Best Cable</td>\n",
       "      <td>1356048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I have used monster cables for years, and with...</td>\n",
       "      <td>01 19, 2014</td>\n",
       "      <td>A1UPZM995ZAH90</td>\n",
       "      <td>Brian</td>\n",
       "      <td>Monster Standard 100 - 21' Instrument Cable</td>\n",
       "      <td>1390089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>I now use this cable to run from the output of...</td>\n",
       "      <td>11 16, 2012</td>\n",
       "      <td>AJNFQI3YR6XJ5</td>\n",
       "      <td>Fender Guy \"Rick\"</td>\n",
       "      <td>Didn't fit my 1996 Fender Strat...</td>\n",
       "      <td>1353024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Perfect for my Epiphone Sheraton II.  Monster ...</td>\n",
       "      <td>07 6, 2008</td>\n",
       "      <td>A3M1PLEYNDEYO8</td>\n",
       "      <td>G. Thomas \"Tom\"</td>\n",
       "      <td>Great cable</td>\n",
       "      <td>1215302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B00004Y2UT</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Monster makes the best cables and a lifetime w...</td>\n",
       "      <td>01 8, 2014</td>\n",
       "      <td>AMNTZU1YQN1TH</td>\n",
       "      <td>Kurt Robair</td>\n",
       "      <td>Best Instrument Cables On The Market</td>\n",
       "      <td>1389139200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  1384719342    [0, 0]        5   \n",
       "1  1384719342  [13, 14]        5   \n",
       "2  1384719342    [1, 1]        5   \n",
       "3  1384719342    [0, 0]        5   \n",
       "4  1384719342    [0, 0]        5   \n",
       "5  B00004Y2UT    [0, 0]        5   \n",
       "6  B00004Y2UT    [0, 0]        5   \n",
       "7  B00004Y2UT    [0, 0]        3   \n",
       "8  B00004Y2UT    [0, 0]        5   \n",
       "9  B00004Y2UT    [0, 0]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Not much to write about here, but it does exac...  02 28, 2014   \n",
       "1  The product does exactly as it should and is q...  03 16, 2013   \n",
       "2  The primary job of this device is to block the...  08 28, 2013   \n",
       "3  Nice windscreen protects my MXL mic and preven...  02 14, 2014   \n",
       "4  This pop filter is great. It looks and perform...  02 21, 2014   \n",
       "5  So good that I bought another one.  Love the h...  12 21, 2012   \n",
       "6  I have used monster cables for years, and with...  01 19, 2014   \n",
       "7  I now use this cable to run from the output of...  11 16, 2012   \n",
       "8  Perfect for my Epiphone Sheraton II.  Monster ...   07 6, 2008   \n",
       "9  Monster makes the best cables and a lifetime w...   01 8, 2014   \n",
       "\n",
       "       reviewerID                                      reviewerName  \\\n",
       "0  A2IBPI20UZIR0U  cassandra tu \"Yeah, well, that's just like, u...   \n",
       "1  A14VAT5EAX3D9S                                              Jake   \n",
       "2  A195EZSQDW3E21                     Rick Bennette \"Rick Bennette\"   \n",
       "3  A2C00NNG1ZQQG2                         RustyBill \"Sunday Rocker\"   \n",
       "4   A94QU4C90B1AX                                     SEAN MASLANKA   \n",
       "5  A2A039TZMZHH9Y                               Bill Lewey \"blewey\"   \n",
       "6  A1UPZM995ZAH90                                             Brian   \n",
       "7   AJNFQI3YR6XJ5                                 Fender Guy \"Rick\"   \n",
       "8  A3M1PLEYNDEYO8                                   G. Thomas \"Tom\"   \n",
       "9   AMNTZU1YQN1TH                                       Kurt Robair   \n",
       "\n",
       "                                       summary  unixReviewTime  \n",
       "0                                         good      1393545600  \n",
       "1                                         Jake      1363392000  \n",
       "2                         It Does The Job Well      1377648000  \n",
       "3                GOOD WINDSCREEN FOR THE MONEY      1392336000  \n",
       "4        No more pops when I record my vocals.      1392940800  \n",
       "5                               The Best Cable      1356048000  \n",
       "6  Monster Standard 100 - 21' Instrument Cable      1390089600  \n",
       "7           Didn't fit my 1996 Fender Strat...      1353024000  \n",
       "8                                  Great cable      1215302400  \n",
       "9         Best Instrument Cables On The Market      1389139200  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# This is the model we'll be using.\n",
    "from sklearn import tree\n",
    "\n",
    "# A convenience for displaying visualizations.\n",
    "from IPython.display import Image\n",
    "\n",
    "# Packages for rendering our tree.\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "\n",
    "path = 'Musical_Instruments_5.json' \n",
    "data = pd.read_json(path, lines=True) \n",
    "df = pd.DataFrame(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin               0\n",
       "helpful            0\n",
       "overall            0\n",
       "reviewText         0\n",
       "reviewTime         0\n",
       "reviewerID         0\n",
       "reviewerName      27\n",
       "summary            0\n",
       "unixReviewTime     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>02 28, 2014</td>\n",
       "      <td>A2IBPI20UZIR0U</td>\n",
       "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
       "      <td>good</td>\n",
       "      <td>1393545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>5</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>03 16, 2013</td>\n",
       "      <td>A14VAT5EAX3D9S</td>\n",
       "      <td>Jake</td>\n",
       "      <td>Jake</td>\n",
       "      <td>1363392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>08 28, 2013</td>\n",
       "      <td>A195EZSQDW3E21</td>\n",
       "      <td>Rick Bennette \"Rick Bennette\"</td>\n",
       "      <td>It Does The Job Well</td>\n",
       "      <td>1377648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>02 14, 2014</td>\n",
       "      <td>A2C00NNG1ZQQG2</td>\n",
       "      <td>RustyBill \"Sunday Rocker\"</td>\n",
       "      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n",
       "      <td>1392336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384719342</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>02 21, 2014</td>\n",
       "      <td>A94QU4C90B1AX</td>\n",
       "      <td>SEAN MASLANKA</td>\n",
       "      <td>No more pops when I record my vocals.</td>\n",
       "      <td>1392940800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  1384719342    [0, 0]        5   \n",
       "1  1384719342  [13, 14]        5   \n",
       "2  1384719342    [1, 1]        5   \n",
       "3  1384719342    [0, 0]        5   \n",
       "4  1384719342    [0, 0]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Not much to write about here, but it does exac...  02 28, 2014   \n",
       "1  The product does exactly as it should and is q...  03 16, 2013   \n",
       "2  The primary job of this device is to block the...  08 28, 2013   \n",
       "3  Nice windscreen protects my MXL mic and preven...  02 14, 2014   \n",
       "4  This pop filter is great. It looks and perform...  02 21, 2014   \n",
       "\n",
       "       reviewerID                                      reviewerName  \\\n",
       "0  A2IBPI20UZIR0U  cassandra tu \"Yeah, well, that's just like, u...   \n",
       "1  A14VAT5EAX3D9S                                              Jake   \n",
       "2  A195EZSQDW3E21                     Rick Bennette \"Rick Bennette\"   \n",
       "3  A2C00NNG1ZQQG2                         RustyBill \"Sunday Rocker\"   \n",
       "4   A94QU4C90B1AX                                     SEAN MASLANKA   \n",
       "\n",
       "                                 summary  unixReviewTime  \n",
       "0                                   good      1393545600  \n",
       "1                                   Jake      1363392000  \n",
       "2                   It Does The Job Well      1377648000  \n",
       "3          GOOD WINDSCREEN FOR THE MONEY      1392336000  \n",
       "4  No more pops when I record my vocals.      1392940800  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10234 entries, 0 to 10260\n",
      "Data columns (total 9 columns):\n",
      "asin              10234 non-null object\n",
      "helpful           10234 non-null object\n",
      "overall           10234 non-null int64\n",
      "reviewText        10234 non-null object\n",
      "reviewTime        10234 non-null object\n",
      "reviewerID        10234 non-null object\n",
      "reviewerName      10234 non-null object\n",
      "summary           10234 non-null object\n",
      "unixReviewTime    10234 non-null int64\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 799.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data.overall.copy()\n",
    "review_data = data.reviewText.copy()\n",
    "summary_data = data.summary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF on review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 20241\n",
      "Percent variance captured by all components: 60.83335565911644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=30000)\n",
    "\n",
    "#Applying the vectorizer\n",
    "tfidf_review_data =word_vectorizer.fit_transform(review_data)\n",
    "print(\"Number of features: %d\" % tfidf_review_data.get_shape()[1])\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 20241 to 450.\n",
    "svd= TruncatedSVD(1000)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "truncated_tfidf_review_data = lsa.fit_transform(tfidf_review_data)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Create our testing and training data set, with spread of 70% in training and 30% in testing\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(truncated_tfidf_data, target, test_size=0.3, random_state=20)\n",
    "\n",
    "# fit and transform on it the training features\n",
    "#word_vectorizer.fit(X_train)\n",
    "#X_train_word_features = word_vectorizer.transform(X_train)\n",
    "\n",
    "#transform the test features to sparse matrix\n",
    "#word_vectorizer.fit(X_test)\n",
    "#test_features = word_vectorizer.transform(X_test)\n",
    "\n",
    "import time\n",
    "start_time = time.time() \n",
    "reset_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF on summary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3983\n",
      "Percent variance captured by all components: 73.52250362295567\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=30000)\n",
    "\n",
    "#Applying the vectorizer\n",
    "tfidf_summary_data =word_vectorizer.fit_transform(summary_data)\n",
    "print(\"Number of features: %d\" % tfidf_summary_data.get_shape()[1])\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 20241 to 450.\n",
    "svd= TruncatedSVD(500)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "truncated_tfidf_summary_data = lsa.fit_transform(tfidf_summary_data)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the two arrays into one data set\n",
    "selected_data = np.concatenate((truncated_tfidf_summary_data, truncated_tfidf_review_data), axis=1)\n",
    "#Create our testing and training data set, with spread of 70% in training and 30% in testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_data, target, test_size=0.3, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronal\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model accuracy\n",
      "0.6991208075545425\n",
      "--- 7.91845703125 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Set up the regression model to predict defaults using all other\n",
    "# variables as features.\n",
    "regr = LogisticRegression(solver='lbfgs')\n",
    "regrfit = regr.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy. First, get probability that each row will be admitted.\n",
    "pred_statsmod = regrfit.predict(X_train)\n",
    "\n",
    "# Code admission as 1 if probability is greater than .5.\n",
    "pred_y_statsmod = np.where(pred_statsmod < .5, 0, 1)\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "print('\\n Model accuracy')\n",
    "score = regr.score(X_test, y_test)\n",
    "print(score)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time - reset_time))\n",
    "reset_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy:  0.6763925729442971\n",
      "--- 109.30828547477722 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "#Create a generic random forest tree\n",
    "rfc = ensemble.RandomForestClassifier(\n",
    "    criterion='entropy',\n",
    "    max_features = 'auto',\n",
    "    max_depth = 2 ,\n",
    "    n_estimators = 2,\n",
    "    n_jobs=-1\n",
    "    )\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'n_estimators': [10, 15, 20],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [1, 2, 3],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "gd_sr = GridSearchCV(estimator=rfc,  \n",
    "                     param_grid=parameters,\n",
    "                     scoring='accuracy',\n",
    "                     cv=5,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "gd_sr.fit(X_train, y_train)  \n",
    "best_result = gd_sr.best_score_  \n",
    "print(\"Best Accuracy: \", best_result)  \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time - reset_time))\n",
    "reset_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'n_estimators': 10,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model Type:  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "With 20% Holdout: 0.6991208075545425\n",
      "Testing on Sample: 0.7540551104162595\n",
      "\n",
      "Cross Validation\n",
      "[0.68780488 0.68261719 0.70019531 0.70410156 0.70996094 0.70898438\n",
      " 0.70772239 0.68914956 0.68786693 0.71498531]\n",
      "\n",
      " Confusion Matrix\n",
      "[[  16    2    2    9   30]\n",
      " [   0    2   19   14   50]\n",
      " [   1    0   79   52  114]\n",
      " [   0    0   12  196  397]\n",
      " [   0    0    3   51 2022]]\n",
      "\n",
      " Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.27      0.42        59\n",
      "           2       0.50      0.02      0.04        85\n",
      "           3       0.69      0.32      0.44       246\n",
      "           4       0.61      0.32      0.42       605\n",
      "           5       0.77      0.97      0.86      2076\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      3071\n",
      "   macro avg       0.70      0.38      0.44      3071\n",
      "weighted avg       0.73      0.75      0.71      3071\n",
      "\n",
      "--- 165.83912754058838 seconds ---\n",
      "\n",
      "\n",
      "Model Type:  GridSearchCV(cv=5, error_score='raise-deprecating',\n",
      "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=2, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "       fit_params=None, iid='warn', n_jobs=-1,\n",
      "       param_grid={'n_estimators': [10, 15, 20], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [1, 2, 3], 'criterion': ['gini', 'entropy'], 'bootstrap': [True, False]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring='accuracy', verbose=0)\n",
      "\n",
      "With 20% Holdout: 0.6760013025073266\n",
      "Testing on Sample: 0.6759820207152628\n",
      "\n",
      "Cross Validation\n",
      "[0.67512195 0.67675781 0.67578125 0.67578125 0.67578125 0.67578125\n",
      " 0.67644184 0.67741935 0.67612524 0.67678746]\n",
      "\n",
      " Confusion Matrix\n",
      "[[   0    0    0    0   59]\n",
      " [   0    0    0    0   85]\n",
      " [   0    0    0    0  246]\n",
      " [   0    0    0    0  605]\n",
      " [   0    0    0    0 2076]]\n",
      "\n",
      " Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        59\n",
      "           2       0.00      0.00      0.00        85\n",
      "           3       0.00      0.00      0.00       246\n",
      "           4       0.00      0.00      0.00       605\n",
      "           5       0.68      1.00      0.81      2076\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      3071\n",
      "   macro avg       0.14      0.20      0.16      3071\n",
      "weighted avg       0.46      0.68      0.55      3071\n",
      "\n",
      "--- 1680.5511651039124 seconds ---\n",
      "\n",
      "\n",
      "Model Type:  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "\n",
      "With 20% Holdout: 0.6802344513187887\n",
      "Testing on Sample: 0.6830173930037131\n",
      "\n",
      "Cross Validation\n",
      "[0.67804878 0.68164062 0.68359375 0.68457031 0.68066406 0.67675781\n",
      " 0.68621701 0.67937439 0.67906067 0.67678746]\n",
      "\n",
      " Confusion Matrix\n",
      "[[   2    0    0    0   57]\n",
      " [   0    1    2    0   82]\n",
      " [   0    0   19    0  227]\n",
      " [   0    0    3    1  601]\n",
      " [   0    0    0    0 2076]]\n",
      "\n",
      " Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.03      0.07        59\n",
      "           2       1.00      0.01      0.02        85\n",
      "           3       0.79      0.08      0.14       246\n",
      "           4       1.00      0.00      0.00       605\n",
      "           5       0.68      1.00      0.81      2076\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      3071\n",
      "   macro avg       0.89      0.22      0.21      3071\n",
      "weighted avg       0.77      0.68      0.56      3071\n",
      "\n",
      "--- 501.4507336616516 seconds ---\n",
      "\n",
      "\n",
      "Model Type:  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "With 20% Holdout: 0.6939107782481276\n",
      "Testing on Sample: 0.7951924956028923\n",
      "\n",
      "Cross Validation\n",
      "[0.69268293 0.67382812 0.70019531 0.703125   0.70605469 0.71289062\n",
      " 0.70381232 0.68914956 0.68688845 0.69147894]\n",
      "\n",
      " Confusion Matrix\n",
      "[[  40    2    4    1   12]\n",
      " [   2   42    9    7   25]\n",
      " [   2    1  109   42   92]\n",
      " [   1    1   13  235  355]\n",
      " [   1    0    6   48 2021]]\n",
      "\n",
      " Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.68      0.76        59\n",
      "           2       0.91      0.49      0.64        85\n",
      "           3       0.77      0.44      0.56       246\n",
      "           4       0.71      0.39      0.50       605\n",
      "           5       0.81      0.97      0.88      2076\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      3071\n",
      "   macro avg       0.81      0.60      0.67      3071\n",
      "weighted avg       0.79      0.80      0.77      3071\n",
      "\n",
      "--- 1303.0805940628052 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "model_names = [regrfit, gd_sr, clf, svclassifier]\n",
    "for model_name in model_names:\n",
    "    print('\\n\\nModel Type: ', model_name)\n",
    "    print('\\nWith 20% Holdout: ' + str(model_name.fit(X_train, y_train).score(X_test, y_test)))\n",
    "    print('Testing on Sample: ' + str(model_name.fit(selected_data, target).score(selected_data, target)))\n",
    "\n",
    "    print('\\nCross Validation')\n",
    "    print(cross_val_score(model_name, selected_data, target, cv=10))\n",
    "\n",
    "    prediction = model_name.predict(X_test)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, prediction)\n",
    "    print('\\n Confusion Matrix')\n",
    "    print(cnf_matrix)\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print('\\n Classification Report')\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time - reset_time))\n",
    "    reset_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of these four models, SVM shows the best accuracy in its model in terms of it's cross validation score as well as in it's classification report. However for it's runtime, Logistic Regression has the lowest, followed by SVM, Random Forest and Gradient Boosting.\n",
    "\n",
    "\n",
    "The reason for why the random forest tree's poor performance is due to the low amount of n-estimators for modeling the data set, ideally a n-estimator value of 500 or more would be ideal, however it is not possible with my hardware to achieve in any reasonable amount of time. It's classification report score also leads me to believe that random forest does not perform very well in terms of modeling text data. This may be due to the low amount of n-estimators, but even when compared to gradient boosting atleast gradient boosting was able to correctly predict a few reviews. Random forests low score might be due to how exact the data set is, and that multiple approaches to the same data set actually induces more variance or wrong assumptions which causes the model to underperform.\n",
    "\n",
    "Logistic regression works really well because of the nature of the problem being a classification problem and that values are either a hit or miss, but with the number of features that we have to experiement with (which captures 60-70% of the variance of summary and reviewText) we were able to capture alot of the data. Compared to SVM however, although it is more accurate in its cross validation, SVM is able to correctly guess the correct review more times then logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM has the best model for this dataset overall, it's modeling method of grouping support vectors and categorizing data into being \"good\" or \"bad\" fits very closely to how the rating is gauged by how many words with positive or negative conotations there are (assuming there are no large amount of misleading reviews).\n",
    "Random Forest does not work as well in this scenerio due this kind of dataset looking for more depth instead of breadth (and that my computer cannot handle a random forest that is too large). In this case Gradient Boosting works much better as it is basically looking for more depth in the data, which allows it to pick up more on the small variances on the dataset more accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
